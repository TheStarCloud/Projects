{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "cd2036d5",
   "metadata": {},
   "outputs": [],
   "source": [
    "from nltk.tokenize import PunktSentenceTokenizer, word_tokenize\n",
    "\n",
    "def word_sentence_tokenize(text):\n",
    "  \n",
    "  # create a PunktSentenceTokenizer\n",
    "  sentence_tokenizer = PunktSentenceTokenizer(text)\n",
    "  \n",
    "  # sentence tokenize text\n",
    "  sentence_tokenized = sentence_tokenizer.tokenize(text)\n",
    "  \n",
    "  # create a list to hold word tokenized sentences\n",
    "  word_tokenized = list()\n",
    "  \n",
    "  # for-loop through each tokenized sentence in sentence_tokenized\n",
    "  for tokenized_sentence in sentence_tokenized:\n",
    "    # word tokenize each sentence and append to word_tokenized\n",
    "    word_tokenized.append(word_tokenize(tokenized_sentence))\n",
    "    \n",
    "  return word_tokenized"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "id": "2648f44a",
   "metadata": {},
   "outputs": [],
   "source": [
    "from collections import Counter\n",
    "\n",
    "# function that pulls chunks out of chunked sentence and finds the most common NP chunks\n",
    "def np_chunk_counter(chunked_sentences):\n",
    "\n",
    "    # create a list to hold chunks\n",
    "    chunks = list()\n",
    "\n",
    "    # for-loop through each chunked sentence to extract noun phrase chunks\n",
    "    for chunked_sentence in chunked_sentences:\n",
    "        for subtree in chunked_sentence.subtrees(filter=lambda t: t.label() == 'NP'):\n",
    "            chunks.append(tuple(subtree))\n",
    "\n",
    "    # create a Counter object\n",
    "    chunk_counter = Counter()\n",
    "\n",
    "    # for-loop through the list of chunks\n",
    "    for chunk in chunks:\n",
    "        # increase counter of specific chunk by 1\n",
    "        chunk_counter[chunk] += 1\n",
    "\n",
    "    # return 30 most frequent chunks\n",
    "    return chunk_counter.most_common(30)\n",
    "\n",
    "\n",
    "# function that pulls chunks out of chunked sentence and finds the most common VP chunks\n",
    "def vp_chunk_counter(chunked_sentences):\n",
    "\n",
    "    # create a list to hold chunks\n",
    "    chunks = list()\n",
    "\n",
    "    # for-loop through each chunked sentence to extract verb phrase chunks\n",
    "    for chunked_sentence in chunked_sentences:\n",
    "        for subtree in chunked_sentence.subtrees(filter=lambda t: t.label() == 'VP'):\n",
    "            chunks.append(tuple(subtree))\n",
    "\n",
    "    # create a Counter object\n",
    "    chunk_counter = Counter()\n",
    "\n",
    "    # for-loop through the list of chunks\n",
    "    for chunk in chunks:\n",
    "        # increase counter of specific chunk by 1\n",
    "        chunk_counter[chunk] += 1\n",
    "\n",
    "    # return 30 most frequent chunks\n",
    "    return chunk_counter.most_common(30)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "id": "ac79b0c2",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['those', 'who', 'read', 'the', 'symbol', 'do', 'so', 'at', 'their', 'peril', '.']\n",
      "[('those', 'DT'), ('who', 'WP'), ('read', 'VBP'), ('the', 'DT'), ('symbol', 'NN'), ('do', 'VBP'), ('so', 'RB'), ('at', 'IN'), ('their', 'PRP$'), ('peril', 'NN'), ('.', '.')]\n",
      "[((('i', 'NN'),), 963), ((('henry', 'NN'),), 200), ((('lord', 'NN'),), 197), ((('life', 'NN'),), 171), ((('harry', 'NN'),), 136), ((('dorian', 'JJ'), ('gray', 'NN')), 128), ((('something', 'NN'),), 126), ((('nothing', 'NN'),), 93), ((('basil', 'NN'),), 85), ((('anything', 'NN'),), 70), ((('the', 'DT'), ('world', 'NN')), 70), ((('everything', 'NN'),), 69), ((('hallward', 'NN'),), 68), ((('the', 'DT'), ('man', 'NN')), 61), ((('the', 'DT'), ('room', 'NN')), 60), ((('face', 'NN'),), 57), ((('the', 'DT'), ('door', 'NN')), 56), ((('love', 'NN'),), 55), ((('art', 'NN'),), 52), ((('course', 'NN'),), 52), ((('the', 'DT'), ('picture', 'NN')), 48), ((('the', 'DT'), ('lad', 'NN')), 45), ((('head', 'NN'),), 44), ((('round', 'NN'),), 44), ((('hand', 'NN'),), 44), ((('sibyl', 'NN'),), 41), ((('the', 'DT'), ('table', 'NN')), 40), ((('the', 'DT'), ('painter', 'NN')), 38), ((('sir', 'NN'),), 38), ((('a', 'DT'), ('moment', 'NN')), 38)]\n",
      "\n",
      "[((('i', 'NN'), ('am', 'VBP')), 101), ((('i', 'NN'), ('was', 'VBD')), 40), ((('i', 'NN'), ('want', 'VBP')), 37), ((('i', 'NN'), ('know', 'VBP')), 33), ((('i', 'NN'), ('do', 'VBP'), (\"n't\", 'RB')), 32), ((('i', 'NN'), ('have', 'VBP')), 32), ((('i', 'NN'), ('had', 'VBD')), 31), ((('i', 'NN'), ('suppose', 'VBP')), 17), ((('i', 'NN'), ('think', 'VBP')), 16), ((('i', 'NN'), ('am', 'VBP'), ('not', 'RB')), 14), ((('i', 'NN'), ('thought', 'VBD')), 13), ((('i', 'NN'), ('believe', 'VBP')), 12), ((('dorian', 'JJ'), ('gray', 'NN'), ('was', 'VBD')), 11), ((('i', 'NN'), ('am', 'VBP'), ('so', 'RB')), 11), ((('henry', 'NN'), ('had', 'VBD')), 11), ((('i', 'NN'), ('did', 'VBD'), (\"n't\", 'RB')), 9), ((('i', 'NN'), ('met', 'VBD')), 9), ((('i', 'NN'), ('said', 'VBD')), 9), ((('i', 'NN'), ('am', 'VBP'), ('quite', 'RB')), 8), ((('i', 'NN'), ('see', 'VBP')), 8), ((('i', 'NN'), ('did', 'VBD'), ('not', 'RB')), 7), ((('i', 'NN'), ('have', 'VBP'), ('ever', 'RB')), 7), ((('life', 'NN'), ('has', 'VBZ')), 7), ((('i', 'NN'), ('did', 'VBD')), 6), ((('i', 'NN'), ('feel', 'VBP')), 6), ((('life', 'NN'), ('is', 'VBZ')), 6), ((('the', 'DT'), ('lad', 'NN'), ('was', 'VBD')), 6), ((('i', 'NN'), ('asked', 'VBD')), 6), ((('i', 'NN'), ('came', 'VBD')), 6), ((('i', 'NN'), ('felt', 'VBD')), 6)]\n"
     ]
    }
   ],
   "source": [
    "import nltk\n",
    "#nltk.download()\n",
    "\n",
    "# import text of choice here\n",
    "text = open(\"dorian_gray.txt\",encoding='utf-8').read().lower()\n",
    "\n",
    "# sentence and word tokenize text here\n",
    "word_tokenized_text = word_sentence_tokenize(text)\n",
    "\n",
    "# store and print any word tokenized sentence here\n",
    "single_word_tokenized_sentence = word_tokenized_text[27]\n",
    "print(single_word_tokenized_sentence)\n",
    "\n",
    "# create a list to hold part-of-speech tagged sentences here\n",
    "pos_tagged_text = []\n",
    "\n",
    "# create a for loop through each word tokenized sentence here\n",
    "for sentence in word_tokenized_text:\n",
    "  \n",
    "  # part-of-speech tag each sentence and append to list of pos-tagged sentences here\n",
    "  pos_tagged_text.append(pos_tag(sentence))\n",
    "\n",
    "# store and print any part-of-speech tagged sentence here\n",
    "single_pos_sentence = pos_tagged_text[27]\n",
    "print(single_pos_sentence)\n",
    "\n",
    "\n",
    "# define noun phrase chunk grammar here\n",
    "np_chunk_grammar = \"NP: {<DT>?<JJ>*<NN>}\"\n",
    "\n",
    "# create noun phrase RegexpParser object here\n",
    "np_chunk_parser = RegexpParser(np_chunk_grammar)\n",
    "\n",
    "# define verb phrase chunk grammar here\n",
    "vp_chunk_grammar = \"VP: {<DT>?<JJ>*<NN><VB.*><RB.?>?}\"\n",
    "\n",
    "# create verb phrase RegexpParser object here\n",
    "vp_chunk_parser = RegexpParser(vp_chunk_grammar)\n",
    "\n",
    "# create a list to hold noun phrase chunked sentences and a list to hold verb phrase chunked sentences here\n",
    "np_chunked_text = []\n",
    "vp_chunked_text = []\n",
    "\n",
    "\n",
    "# create a for loop through each pos-tagged sentence here\n",
    "for sentence in pos_tagged_text:\n",
    "  # chunk each sentence and append to lists here\n",
    "  np_chunked_text.append(np_chunk_parser.parse(sentence))\n",
    "  vp_chunked_text.append(vp_chunk_parser.parse(sentence))  \n",
    "\n",
    "# store and print the most common NP-chunks here\n",
    "most_common_np_chunks = np_chunk_counter(np_chunked_text)\n",
    "print(most_common_np_chunks), print()\n",
    "\n",
    "# store and print the most common VP-chunks here\n",
    "most_common_vp_chunks = vp_chunk_counter(vp_chunked_text)\n",
    "print(most_common_vp_chunks)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3b36f50a",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.10"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
